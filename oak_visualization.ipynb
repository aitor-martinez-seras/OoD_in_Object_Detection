{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict, defaultdict\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "root_path =  Path(Path.cwd()).resolve()\n",
    "owod_tasks_path = root_path / 'datasets_utils' / 'owod' / 'tasks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(owod_tasks_path / 't1_train.txt', 'r') as f:\n",
    "    train_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/groups/tri110414/yolo-pruebas/datasets_utils/owod/tasks/t1_train.txt')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owod_tasks_path / 't1_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'  # 'train' or 'val'\n",
    "video_number = 1  # 0-199 in train, 0 to 6 in val\n",
    "frame_position_number = 0  # from the first (0) to last frame from each video..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to count the number of boxes per class\n",
    "root_path =  Path(\"/home/tri110414/nfs_home/datasets/OAK/\")\n",
    "images_folder_path = root_path / \"images\" / split\n",
    "labels_folder_path = root_path / \"labels\" / split\n",
    "videos_folder_path = root_path / split / \"Videos\" \n",
    "\n",
    "classes_json_path = 'datasets_utils/oak/oak_classes.json'\n",
    "classes_dict = json.load(open(classes_json_path))\n",
    "\n",
    "video_names_list = [d.name for d in sorted(images_folder_path.iterdir()) if d.is_dir()]\n",
    "video_name = video_names_list[video_number]\n",
    "\n",
    "frames_path_list = sorted((images_folder_path / video_name).iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of annotated frames w.r.t. the total number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: 20140913_172950_458. % of annotated frames: 1.57% (397/25361)\n",
      "Video: 20140930_114708_495. % of annotated frames: 1.57% (290/18488)\n",
      "Video: 20141001_163756_772. % of annotated frames: 1.56% (260/16630)\n",
      "Video: 20141004_171007_314. % of annotated frames: 1.57% (594/37869)\n",
      "Video: 20141006_132312_894. % of annotated frames: 1.55% (450/29036)\n",
      "Video: 20141008_154205_152. % of annotated frames: 1.56% (489/31248)\n",
      "Video: 20141009_185447_957. % of annotated frames: 1.57% (299/19031)\n",
      "Video: 20141011_120104_835. % of annotated frames: 1.54% (420/27237)\n",
      "Video: 20141011_155639_773. % of annotated frames: 1.57% (267/17056)\n",
      "Video: 20141013_180629_736. % of annotated frames: 1.57% (392/25046)\n",
      "Video: 20141015_161746_653. % of annotated frames: 1.57% (552/35189)\n",
      "Video: 20141016_180651_373. % of annotated frames: 1.56% (423/27032)\n",
      "Video: 20141019_150508_959. % of annotated frames: 1.57% (764/48704)\n",
      "Video: 20141019_155555_270. % of annotated frames: 1.56% (282/18045)\n",
      "Video: 20141019_164919_101. % of annotated frames: 1.56% (288/18414)\n",
      "Video: 20141019_173733_820. % of annotated frames: 1.54% (417/27045)\n",
      "Video: 20141019_175533_029. % of annotated frames: 1.45% (997/68847)\n",
      "Video: 20141023_120402_424. % of annotated frames: 1.57% (810/51691)\n",
      "Video: 20141025_175748_280. % of annotated frames: 1.57% (846/53977)\n",
      "Video: 20141026_183826_414. % of annotated frames: 1.17% (279/23852)\n",
      "Video: 20141027_131547_425. % of annotated frames: 1.56% (218/13948)\n",
      "Video: 20141028_115722_211. % of annotated frames: 1.57% (231/14708)\n",
      "Video: 20141028_145142_772. % of annotated frames: 1.56% (296/18968)\n",
      "Video: 20141030_135114_847. % of annotated frames: 1.57% (959/61143)\n",
      "Video: 20141030_143828_844. % of annotated frames: 1.51% (275/18207)\n",
      "Video: 20141103_094911_514. % of annotated frames: 1.56% (269/17195)\n",
      "Video: 20141120_131923_454. % of annotated frames: 1.57% (344/21901)\n",
      "Video: 20141126_113108_948. % of annotated frames: 1.56% (203/12994)\n",
      "Video: 20141129_142849_812. % of annotated frames: 1.56% (249/15914)\n",
      "Video: 20141216_151916_518. % of annotated frames: 1.57% (255/16215)\n",
      "Video: 20141219_124857_320. % of annotated frames: 1.56% (206/13176)\n",
      "Video: 20150106_122652_076. % of annotated frames: 1.56% (295/18863)\n",
      "Video: 20150109_132444_725. % of annotated frames: 1.56% (318/20335)\n",
      "Video: 20150112_140104_676. % of annotated frames: 1.57% (246/15692)\n",
      "Video: 20150119_131825_726. % of annotated frames: 1.56% (204/13070)\n",
      "Video: 20150120_161845_689. % of annotated frames: 1.57% (306/19508)\n",
      "Video: 20150121_132745_747. % of annotated frames: 1.52% (204/13418)\n",
      "Video: 20150125_123506_436. % of annotated frames: 1.57% (671/42781)\n",
      "Video: 20150127_135442_228. % of annotated frames: 1.57% (232/14801)\n",
      "Video: 20150130_160543_649. % of annotated frames: 1.56% (236/15119)\n",
      "Video: 20150203_120911_968. % of annotated frames: 1.57% (198/12603)\n",
      "Video: 20150205_134757_620. % of annotated frames: 1.56% (361/23079)\n",
      "Video: 20150208_160238_593. % of annotated frames: 1.51% (475/31548)\n",
      "Video: 20150213_115906_604. % of annotated frames: 1.56% (308/19734)\n",
      "Video: 20150222_164908_256. % of annotated frames: 1.57% (337/21497)\n",
      "Video: 20150312_160458_634. % of annotated frames: 1.57% (242/15429)\n",
      "Video: 20150316_133901_259. % of annotated frames: 1.57% (396/25278)\n",
      "Video: 20150317_125902_845. % of annotated frames: 1.40% (255/18273)\n",
      "Video: 20150317_160829_517. % of annotated frames: 1.57% (273/17432)\n",
      "Video: 20150324_173951_618. % of annotated frames: 1.57% (250/15954)\n",
      "Video: 20150328_151014_887. % of annotated frames: 1.56% (333/21330)\n",
      "Video: 20150329_175149_002. % of annotated frames: 1.57% (462/29482)\n",
      "Video: 20150401_182419_207. % of annotated frames: 1.57% (668/42628)\n",
      "Video: 20150406_095444_928. % of annotated frames: 1.57% (945/60292)\n",
      "Video: 20150408_145203_458. % of annotated frames: 1.56% (220/14065)\n",
      "Video: 20150410_151542_380. % of annotated frames: 1.57% (230/14682)\n",
      "Video: 20150411_134309_928. % of annotated frames: 1.57% (291/18577)\n",
      "Video: 20150426_143037_240. % of annotated frames: 1.57% (327/20868)\n",
      "Video: 20150430_162438_129. % of annotated frames: 1.56% (258/16517)\n",
      "Video: 20150503_143344_987. % of annotated frames: 1.57% (392/24966)\n",
      "Video: 20150507_150126_249. % of annotated frames: 1.56% (284/18172)\n",
      "Video: 20150509_180025_001. % of annotated frames: 1.36% (351/25724)\n",
      "Video: 20150509_181947_468. % of annotated frames: 1.57% (423/26961)\n",
      "Video: 20150510_172810_868. % of annotated frames: 1.56% (215/13794)\n",
      "Video: 20150512_185333_513. % of annotated frames: 1.57% (685/43717)\n",
      "Video: 20150512_191941_300. % of annotated frames: 1.44% (440/30613)\n",
      "Video: 20150514_202056_653. % of annotated frames: 1.57% (231/14731)\n",
      "Video: 20150515_185329_516. % of annotated frames: 1.57% (685/43712)\n",
      "Video: 20150518_151546_617. % of annotated frames: 1.56% (222/14243)\n",
      "Video: 20150518_200236_527. % of annotated frames: 1.57% (533/33966)\n",
      "Video: 20150520_155106_930. % of annotated frames: 1.00% (455/45686)\n",
      "Video: 20150520_161709_785. % of annotated frames: 1.57% (816/52056)\n",
      "Video: 20150521_184244_382. % of annotated frames: 1.57% (587/37446)\n",
      "Video: 20150521_200226_847. % of annotated frames: 1.57% (528/33717)\n",
      "Video: 20150522_133013_148. % of annotated frames: 1.57% (485/30925)\n",
      "Video: 20150523_210244_184. % of annotated frames: 1.57% (405/25848)\n",
      "Video: 20150524_205525_707. % of annotated frames: 1.17% (176/15050)\n",
      "Video: 20150524_211347_982. % of annotated frames: 1.57% (488/31163)\n",
      "Video: 20150525_172148_550. % of annotated frames: 1.52% (254/16656)\n",
      "Video: 20150525_191858_977. % of annotated frames: 1.57% (721/45968)\n"
     ]
    }
   ],
   "source": [
    "# Check the number of annotated frames w.r.t. the total number of frames\n",
    "for video_name in video_names_list:\n",
    "    frames_path_list = sorted((images_folder_path / video_name).iterdir())\n",
    "\n",
    "    # Get the video path\n",
    "    video_path = videos_folder_path / f\"{video_name}.mp4\"\n",
    "    cap = cv2.VideoCapture(video_path.as_posix())\n",
    "    # Read the total number of frames\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # print(f\"Video: {video_name}\")\n",
    "    # print(f\"Number of frames: {len(frames_path_list)}\")\n",
    "    # print(f'Total number of frames: {total_frames}')\n",
    "    # print(f'Percentage of annotated frames: {len(frames_path_list)/total_frames*100:.2f}%')\n",
    "    # print()\n",
    "    \n",
    "    print(f'Video: {video_name}. % of annotated frames: {len(frames_path_list)/total_frames*100:.2f}% ({len(frames_path_list)}/{total_frames})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check ocurrence vs prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aeroplane': 0,\n",
       " 'bicycle': 1,\n",
       " 'bird': 2,\n",
       " 'boat': 3,\n",
       " 'bottle': 4,\n",
       " 'bus': 5,\n",
       " 'car': 6,\n",
       " 'cat': 7,\n",
       " 'chair': 8,\n",
       " 'cow': 9,\n",
       " 'dining table': 10,\n",
       " 'dog': 11,\n",
       " 'horse': 12,\n",
       " 'motorcycle': 13,\n",
       " 'person': 14,\n",
       " 'potted plant': 15,\n",
       " 'sheep': 16,\n",
       " 'sofa': 17,\n",
       " 'train': 18,\n",
       " 'monitor': 19,\n",
       " 'stroller': 20,\n",
       " 'cabinet': 21,\n",
       " 'door': 22,\n",
       " 'curtain': 23,\n",
       " 'painting': 24,\n",
       " 'shelf': 25,\n",
       " 'transformer': 26,\n",
       " 'fence': 27,\n",
       " 'desk': 28,\n",
       " 'bridge': 29,\n",
       " 'lamp': 30,\n",
       " 'dome': 31,\n",
       " 'railing': 32,\n",
       " 'cushion': 33,\n",
       " 'box': 34,\n",
       " 'column': 35,\n",
       " 'signboard': 36,\n",
       " 'tactile paving': 37,\n",
       " 'counter': 38,\n",
       " 'sink': 39,\n",
       " 'barrier': 40,\n",
       " 'refrigerator': 41,\n",
       " 'stairs': 42,\n",
       " 'case': 43,\n",
       " 'crutch': 44,\n",
       " 'graffiti': 45,\n",
       " 'coffee table': 46,\n",
       " 'toilet': 47,\n",
       " 'book': 48,\n",
       " 'bench': 49,\n",
       " 'road barrier gate': 50,\n",
       " 'palm': 51,\n",
       " 'fruit': 52,\n",
       " 'computer': 53,\n",
       " 'arcade machine': 54,\n",
       " 'parking meter': 55,\n",
       " 'light': 56,\n",
       " 'truck': 57,\n",
       " 'awning': 58,\n",
       " 'streetlight': 59,\n",
       " 'booth': 60,\n",
       " 'shopping cart': 61,\n",
       " 'apparel': 62,\n",
       " 'ottoman': 63,\n",
       " 'van': 64,\n",
       " 'gas bottle': 65,\n",
       " 'fountain': 66,\n",
       " 'zebra crossing': 67,\n",
       " 'toy': 68,\n",
       " 'stool': 69,\n",
       " 'basket': 70,\n",
       " 'bag': 71,\n",
       " 'scooter': 72,\n",
       " 'slide': 73,\n",
       " 'ball': 74,\n",
       " 'food': 75,\n",
       " 'tennis court': 76,\n",
       " 'pot': 77,\n",
       " 'construction vehicles': 78,\n",
       " 'sculpture': 79,\n",
       " 'vase': 80,\n",
       " 'traffic light': 81,\n",
       " 'trashcan': 82,\n",
       " 'fan': 83,\n",
       " 'plate': 84,\n",
       " 'bulletin board': 85,\n",
       " 'radiator': 86,\n",
       " 'cup': 87,\n",
       " 'clock': 88,\n",
       " 'flag': 89,\n",
       " 'hot dog': 90,\n",
       " 'manhole': 91,\n",
       " 'fireplug': 92,\n",
       " 'umbrella': 93,\n",
       " 'gravestone': 94,\n",
       " 'air conditioner': 95,\n",
       " 'mailbox': 96,\n",
       " 'push plate actuator': 97,\n",
       " 'knife': 98,\n",
       " 'phone': 99,\n",
       " 'fork': 100,\n",
       " 'waiting shed': 101,\n",
       " 'spoon': 102,\n",
       " 'faucet': 103,\n",
       " 'vending machine': 104,\n",
       " 'frisbee': 105,\n",
       " 'banana': 106,\n",
       " 'balloon': 107,\n",
       " 'wheelchair': 108,\n",
       " 'windmill': 109,\n",
       " 'trafficcone': 110,\n",
       " 'ship': 111,\n",
       " 'pillow': 112}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to load the annotations of each frame of every video and check the classes present in each frame\n",
    "# We have to maintain a dictionary with the info that is present in the previous frame\n",
    "# Then, check the differences between the previous and the current frame and update the dictionary\n",
    "# When 1 class appears, we have to add it to the dictionary a count of 1. If the class is already present, we have to add 1 to the count.\n",
    "# When 1 class disappears, we have to add the obtained count to the final dictionary and restart the count of that class to 0.\n",
    "\n",
    "# Final dict where each position of the list is one ocurrence of the class in the video\n",
    "#   and the value is the number of frames that the class is present in the video in that ocurrence\n",
    "ocurrence_vs_prevalence = {}\n",
    "for k in classes_dict.values():\n",
    "    ocurrence_vs_prevalence[k] = []\n",
    "\n",
    "# Current frame info dict\n",
    "current_frame_info = {}\n",
    "for k in classes_dict.values():\n",
    "    current_frame_info[k] = 0\n",
    "\n",
    "# We have to load the annotations of each frame of every video and check the classes present in each frame\n",
    "for idx_video, video_name in enumerate(video_names_list):\n",
    "\n",
    "    # Get the folder path of the labels corresponding to the video\n",
    "    labels_path = labels_folder_path / video_name\n",
    "    # Get the list of the labels files\n",
    "    labels_files_list = sorted(labels_path.glob('*.txt'))\n",
    "    # Load the annotations of each frame of the video\n",
    "    for idx_frame, label_file in enumerate(labels_files_list):\n",
    "        with open(label_file) as f:\n",
    "            annotations = f.readlines()\n",
    "        for ann in annotations:\n",
    "            idx_class = int(ann.split()[0])\n",
    "            current_frame_info[idx_class] += 1\n",
    "        \n",
    "    break\n",
    "        # # Get the frame number\n",
    "        # frame_number = int(label_file.stem)\n",
    "        # # Get the video path\n",
    "        # video_path = videos_folder_path / f\"{video_name}.mp4\"\n",
    "        # cap = cv2.VideoCapture(video_path.as_posix())\n",
    "        # # Read the total number of frames\n",
    "        # total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        # # Read the frame\n",
    "        # cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        # ret, frame = cap.read()\n",
    "        # # Release the video capture object\n",
    "        # cap.release()\n",
    "\n",
    "        # # Get the classes present in the current frame\n",
    "        # classes_present = [classes_dict[str(annotation['class_id'])] for annotation in annotations]\n",
    "        # # Get the number of boxes per class\n",
    "        # number_of_boxes_per_class = defaultdict(int)\n",
    "        # for class_name in classes_present:\n",
    "        #     number_of_boxes_per_class[class_name] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0,\n",
       " 2: 0,\n",
       " 3: 0,\n",
       " 4: 1,\n",
       " 5: 1,\n",
       " 6: 2,\n",
       " 7: 0,\n",
       " 8: 1,\n",
       " 9: 0,\n",
       " 10: 2,\n",
       " 11: 0,\n",
       " 12: 0,\n",
       " 13: 0,\n",
       " 14: 2451,\n",
       " 15: 0,\n",
       " 16: 0,\n",
       " 17: 0,\n",
       " 18: 0,\n",
       " 19: 0,\n",
       " 20: 25,\n",
       " 21: 0,\n",
       " 22: 1,\n",
       " 23: 0,\n",
       " 24: 0,\n",
       " 25: 4,\n",
       " 26: 2,\n",
       " 27: 454,\n",
       " 28: 0,\n",
       " 29: 0,\n",
       " 30: 1,\n",
       " 31: 6,\n",
       " 32: 0,\n",
       " 33: 23,\n",
       " 34: 0,\n",
       " 35: 1,\n",
       " 36: 136,\n",
       " 37: 0,\n",
       " 38: 0,\n",
       " 39: 0,\n",
       " 40: 0,\n",
       " 41: 0,\n",
       " 42: 19,\n",
       " 43: 0,\n",
       " 44: 0,\n",
       " 45: 2,\n",
       " 46: 0,\n",
       " 47: 0,\n",
       " 48: 0,\n",
       " 49: 7,\n",
       " 50: 0,\n",
       " 51: 0,\n",
       " 52: 0,\n",
       " 53: 0,\n",
       " 54: 3,\n",
       " 55: 0,\n",
       " 56: 60,\n",
       " 57: 0,\n",
       " 58: 131,\n",
       " 59: 122,\n",
       " 60: 4,\n",
       " 61: 0,\n",
       " 62: 0,\n",
       " 63: 0,\n",
       " 64: 0,\n",
       " 65: 0,\n",
       " 66: 0,\n",
       " 67: 0,\n",
       " 68: 0,\n",
       " 69: 0,\n",
       " 70: 0,\n",
       " 71: 89,\n",
       " 72: 0,\n",
       " 73: 0,\n",
       " 74: 0,\n",
       " 75: 0,\n",
       " 76: 0,\n",
       " 77: 0,\n",
       " 78: 0,\n",
       " 79: 0,\n",
       " 80: 0,\n",
       " 81: 0,\n",
       " 82: 51,\n",
       " 83: 0,\n",
       " 84: 0,\n",
       " 85: 101,\n",
       " 86: 0,\n",
       " 87: 0,\n",
       " 88: 0,\n",
       " 89: 17,\n",
       " 90: 0,\n",
       " 91: 0,\n",
       " 92: 0,\n",
       " 93: 11,\n",
       " 94: 0,\n",
       " 95: 0,\n",
       " 96: 0,\n",
       " 97: 0,\n",
       " 98: 0,\n",
       " 99: 0,\n",
       " 100: 0,\n",
       " 101: 0,\n",
       " 102: 0,\n",
       " 103: 0,\n",
       " 104: 0,\n",
       " 105: 0,\n",
       " 106: 0,\n",
       " 107: 0,\n",
       " 108: 2,\n",
       " 109: 0,\n",
       " 110: 0,\n",
       " 111: 0,\n",
       " 112: 0}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_frame_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_frame_info = {}\n",
    "for k in classes_dict.values():\n",
    "    current_frame_info[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14 0.3142361111111111 0.5023148148148148 0.03298611111111111 0.18055555555555555\\n',\n",
       " '14 0.3350694444444444 0.5162037037037037 0.04513888888888889 0.18055555555555555\\n',\n",
       " '14 0.4448784722222222 0.6574074074074074 0.20052083333333334 0.6851851851851852\\n',\n",
       " '71 0.4454752604166667 0.6220100308641974 0.13346354166666666 0.36747685185185175\\n',\n",
       " '27 0.7162905092592595 0.5546553497942387 0.31756365740740744 0.23019547325102874\\n']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'annotations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mannotations\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'annotations' is not defined"
     ]
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check which frames of the videos correspond to the annotated images (1 annotated frame every 60 aproximately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image from .jpg file\n",
    "images_list = []\n",
    "for frame_image in frames_path_list:\n",
    "    print(frame_image)\n",
    "    images_list.append(np.array(Image.open(frame_image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59: Found frame corresponding to 00002 (59)\n",
      "119: Found frame corresponding to 00004 (119)\n",
      "179: Found frame corresponding to 00006 (179)\n",
      "299: Found frame corresponding to 00010 (299)\n",
      "359: Found frame corresponding to 00012 (359)\n",
      "419: Found frame corresponding to 00014 (419)\n",
      "479: Found frame corresponding to 00016 (479)\n",
      "539: Found frame corresponding to 00018 (539)\n",
      "599: Found frame corresponding to 00020 (599)\n",
      "659: Found frame corresponding to 00022 (659)\n",
      "719: Found frame corresponding to 00024 (719)\n",
      "779: Found frame corresponding to 00026 (779)\n",
      "839: Found frame corresponding to 00028 (839)\n",
      "899: Found frame corresponding to 00030 (899)\n",
      "959: Found frame corresponding to 00032 (959)\n",
      "1019: Found frame corresponding to 00034 (1019)\n",
      "1079: Found frame corresponding to 00036 (1079)\n",
      "1139: Found frame corresponding to 00038 (1139)\n",
      "1199: Found frame corresponding to 00040 (1199)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m image \u001b[38;5;241m=\u001b[39m images_list[idx_frame]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[0;32m---> 13\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Break the loop if there are no frames left to read\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 2: Load the video file\n",
    "video_path = videos_folder_path / f'{video_name}.mp4'\n",
    "cap = cv2.VideoCapture(video_path.as_posix())\n",
    "\n",
    "frame_number = 0\n",
    "idx_frame = 0\n",
    "selected_frames = [64, 96, 128, 160, 192] # Example frame numbers to visualize\n",
    "selected_frames = list(np.arange(40, 1500, 1))\n",
    "\n",
    "image = images_list[idx_frame]\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Break the loop if there are no frames left to read\n",
    "    \n",
    "    # Step 4: Check if the current frame is one of the selected frames\n",
    "    #if frame_number in selected_frames:\n",
    "    # Step 5: Convert the frame from BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(frame_rgb)  # Convert to PIL Image\n",
    "\n",
    "    # Convert to numpy array\n",
    "    img = np.array(img)\n",
    "\n",
    "    if (img == image).all():\n",
    "        step_number = frames_path_list[idx_frame].stem[-5:]\n",
    "        print(f'{frame_number}: Found frame corresponding to {step_number} ({(int(step_number) * 30)-1})')\n",
    "        idx_frame += 1\n",
    "        image = images_list[idx_frame]\n",
    "\n",
    "        # Compare with another image\n",
    "        \n",
    "        # # Step 6: Visualize the frame\n",
    "        # plt.figure(figsize=(10, 5))\n",
    "        # plt.imshow(img)\n",
    "        # plt.title(f'Frame {frame_number}')\n",
    "        # plt.axis('off')  # Hide axis\n",
    "        # plt.show()\n",
    "    \n",
    "    frame_number += 1\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n",
      "363\n"
     ]
    }
   ],
   "source": [
    "video_images_path = images_path / video_name\n",
    "video_labels_path = labels_path / video_name\n",
    "\n",
    "print(len(list(video_images_path.iterdir())))\n",
    "print(len(list(video_labels_path.iterdir())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n"
     ]
    }
   ],
   "source": [
    "print(len(list((root_path / 'val' / 'Raw' / video_name).iterdir())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(list((root_path / 'images' / 'val').iterdir())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
